{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Tensorflow Serving on Google Colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivam-kotwalia/data-science-project/blob/master/Tensorflow%20Serving%20on%20Google%20Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZTmnPXW9g5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po077te89v9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "afec5c70-eb65-43e0-c262-ab1b764b0256"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S3AkQsK-Pt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLFP8Oze-Wg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "e92b2f83-bdee-4748-a53e-fab40f6b11b6"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpXXPENT-ZhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e3d4c436-9c5d-405c-91be-dd9f7d4da640"
      },
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGa23u92DG8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0\n",
        "y_train = to_categorical(y_train, num_classes=10, )\n",
        "y_test  = to_categorical(y_test,  num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zLnKXODd0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykjAhJwz-edc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models      import Sequential\n",
        "from tensorflow.keras.layers      import Flatten, Dense, Conv2D, Conv1D\n",
        "from tensorflow.keras.losses      import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers  import SGD\n",
        "from tensorflow.keras.activations import relu, softmax\n",
        "from tensorflow.keras.utils       import to_categorical\n",
        "from tensorflow.keras.models      import save_model\n",
        "import numpy as np \n",
        "import os "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM47mDmIDiCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9MmjpFL_IUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(8, kernel_size=(3, 3), input_shape=(28, 28, 1), activation=relu))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation=softmax))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krvr4FjT_fQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "cc93e96a-e1bc-4137-afab-4e20c837b9e9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5408)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                54090     \n",
            "=================================================================\n",
            "Total params: 54,170\n",
            "Trainable params: 54,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz8Cah6e_hzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\"rmsprop\", categorical_crossentropy, metrics=[\"acc\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1u2MKV1BDMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f4a08dcb-2949-40f8-a795-2f40f195b635"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (60000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX9nvVhO_sgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "dbb310bc-286e-463d-b881-717d18524761"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=20, validation_split=0.2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 1.2398 - acc: 0.6435 - val_loss: 0.7204 - val_acc: 0.7405\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.6613 - acc: 0.7620 - val_loss: 0.5997 - val_acc: 0.7800\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.5854 - acc: 0.7891 - val_loss: 0.5577 - val_acc: 0.7986\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.5482 - acc: 0.8046 - val_loss: 0.5323 - val_acc: 0.8086\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.5254 - acc: 0.8143 - val_loss: 0.5203 - val_acc: 0.8147\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.5087 - acc: 0.8202 - val_loss: 0.5083 - val_acc: 0.8152\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4950 - acc: 0.8266 - val_loss: 0.4946 - val_acc: 0.8217\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4835 - acc: 0.8311 - val_loss: 0.4797 - val_acc: 0.8318\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4732 - acc: 0.8345 - val_loss: 0.4701 - val_acc: 0.8325\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4646 - acc: 0.8396 - val_loss: 0.4646 - val_acc: 0.8353\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 14s 9ms/step - loss: 0.4570 - acc: 0.8418 - val_loss: 0.4575 - val_acc: 0.8389\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4501 - acc: 0.8447 - val_loss: 0.4501 - val_acc: 0.8419\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4438 - acc: 0.8470 - val_loss: 0.4518 - val_acc: 0.8377\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 14s 10ms/step - loss: 0.4386 - acc: 0.8482 - val_loss: 0.4484 - val_acc: 0.8430\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4339 - acc: 0.8512 - val_loss: 0.4448 - val_acc: 0.8449\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4294 - acc: 0.8525 - val_loss: 0.4364 - val_acc: 0.8453\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4255 - acc: 0.8542 - val_loss: 0.4389 - val_acc: 0.8482\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4217 - acc: 0.8561 - val_loss: 0.4305 - val_acc: 0.8490\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4184 - acc: 0.8574 - val_loss: 0.4246 - val_acc: 0.8547\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 15s 10ms/step - loss: 0.4152 - acc: 0.8586 - val_loss: 0.4232 - val_acc: 0.8544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f22b00820b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbksGOFJ_zr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c89452c4-e2ab-4364-f5df-700b9c04643e"
      },
      "source": [
        "VERSION = 1\n",
        "!mkdir -p save_model/{VERSION}\n",
        "save_model(model, os.path.join(\"save_model\", str(VERSION)))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: save_model/1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmad8LNaAkNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a7e1ea8-5bd1-4228-fd0b-5a9d501f96b0"
      },
      "source": [
        "#Currently you saved the model so you know the input and output but if you would have got the model from someone this would have been difficult\n",
        "# So we have a CLI from TensorFlow to cheek MetaGraphDefs & Sugnature Defs\n",
        "!saved_model_cli show --dir \"save_model\" --all"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['conv2d_3_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_conv2d_3_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense_2'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0404 21:29:55.792284 139999391315840 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'conv2d_3_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          conv2d_3_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'conv2d_3_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=u'inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUbsLWycGZvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "03d663cb-7c53-43dd-f933-a24716825ef9"
      },
      "source": [
        "# Download the TensorFlow Serving \n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl -s https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt -qq update "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "OK\n",
            "58 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYlIe2ljHLOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "2b6276de-da05-49da-b517-b512e7346347"
      },
      "source": [
        "!apt-get -qq install  tensorflow-model-server"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package tensorflow-model-server.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 133872 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.1.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.1.0) ...\n",
            "Setting up tensorflow-model-server (2.1.0) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC5Hg1uCH4GX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "df11b00b-c928-48f0-f3f6-882d5bffcd68"
      },
      "source": [
        "! tensorflow_model_server  --help"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: tensorflow_model_server\n",
            "Flags:\n",
            "\t--port=8500                      \tint32\tPort to listen on for gRPC API\n",
            "\t--grpc_socket_path=\"\"            \tstring\tIf non-empty, listen to a UNIX socket for gRPC API on the given path. Can be either relative or absolute path.\n",
            "\t--rest_api_port=0                \tint32\tPort to listen on for HTTP/REST API. If set to zero HTTP/REST API will not be exported. This port must be different than the one specified in --port.\n",
            "\t--rest_api_num_threads=8         \tint32\tNumber of threads for HTTP/REST API processing. If not set, will be auto set based on number of CPUs.\n",
            "\t--rest_api_timeout_in_ms=30000   \tint32\tTimeout for HTTP/REST API calls.\n",
            "\t--enable_batching=false          \tbool\tenable batching\n",
            "\t--allow_version_labels_for_unavailable_models=false\tbool\tIf true, allows assigning unused version labels to models that are not available yet.\n",
            "\t--batching_parameters_file=\"\"    \tstring\tIf non-empty, read an ascii BatchingParameters protobuf from the supplied file name and use the contained values instead of the defaults.\n",
            "\t--model_config_file=\"\"           \tstring\tIf non-empty, read an ascii ModelServerConfig protobuf from the supplied file name, and serve the models in that file. This config file can be used to specify multiple models to serve and other advanced parameters including non-default version policy. (If used, --model_name, --model_base_path are ignored.)\n",
            "\t--model_config_file_poll_wait_seconds=0\tint32\tInterval in seconds between each poll of the filesystemfor model_config_file. If unset or set to zero, poll will be done exactly once and not periodically. Setting this to negative is reserved for testing purposes only.\n",
            "\t--model_name=\"default\"           \tstring\tname of model (ignored if --model_config_file flag is set)\n",
            "\t--model_base_path=\"\"             \tstring\tpath to export (ignored if --model_config_file flag is set, otherwise required)\n",
            "\t--max_num_load_retries=5         \tint32\tmaximum number of times it retries loading a model after the first failure, before giving up. If set to 0, a load is attempted only once. Default: 5\n",
            "\t--load_retry_interval_micros=60000000\tint64\tThe interval, in microseconds, between each servable load retry. If set negative, it doesn't wait. Default: 1 minute\n",
            "\t--file_system_poll_wait_seconds=1\tint32\tInterval in seconds between each poll of the filesystem for new model version. If set to zero poll will be exactly done once and not periodically. Setting this to negative value will disable polling entirely causing ModelServer to indefinitely wait for a new model at startup. Negative values are reserved for testing purposes only.\n",
            "\t--flush_filesystem_caches=true   \tbool\tIf true (the default), filesystem caches will be flushed after the initial load of all servables, and after each subsequent individual servable reload (if the number of load threads is 1). This reduces memory consumption of the model server, at the potential cost of cache misses if model files are accessed after servables are loaded.\n",
            "\t--tensorflow_session_parallelism=0\tint64\tNumber of threads to use for running a Tensorflow session. Auto-configured by default.Note that this option is ignored if --platform_config_file is non-empty.\n",
            "\t--tensorflow_intra_op_parallelism=0\tint64\tNumber of threads to use to parallelize the executionof an individual op. Auto-configured by default.Note that this option is ignored if --platform_config_file is non-empty.\n",
            "\t--tensorflow_inter_op_parallelism=0\tint64\tControls the number of operators that can be executed simultaneously. Auto-configured by default.Note that this option is ignored if --platform_config_file is non-empty.\n",
            "\t--ssl_config_file=\"\"             \tstring\tIf non-empty, read an ascii SSLConfig protobuf from the supplied file name and set up a secure gRPC channel\n",
            "\t--platform_config_file=\"\"        \tstring\tIf non-empty, read an ascii PlatformConfigMap protobuf from the supplied file name, and use that platform config instead of the Tensorflow platform. (If used, --enable_batching is ignored.)\n",
            "\t--per_process_gpu_memory_fraction=0.000000\tfloat\tFraction that each process occupies of the GPU memory space the value is between 0.0 and 1.0 (with 0.0 as the default) If 1.0, the server will allocate all the memory when the server starts, If 0.0, Tensorflow will automatically select a value.\n",
            "\t--saved_model_tags=\"serve\"       \tstring\tComma-separated set of tags corresponding to the meta graph def to load from SavedModel.\n",
            "\t--grpc_channel_arguments=\"\"      \tstring\tA comma separated list of arguments to be passed to the grpc server. (e.g. grpc.max_connection_age_ms=2000)\n",
            "\t--enable_model_warmup=true       \tbool\tEnables model warmup, which triggers lazy initializations (such as TF optimizations) at load time, to reduce first request latency.\n",
            "\t--version=false                  \tbool\tDisplay version\n",
            "\t--monitoring_config_file=\"\"      \tstring\tIf non-empty, read an ascii MonitoringConfig protobuf from the supplied file name\n",
            "\t--remove_unused_fields_from_bundle_metagraph=true\tbool\tRemoves unused fields from MetaGraphDef proto message to save memory.\n",
            "\t--use_tflite_model=false         \tbool\tEXPERIMENTAL; CAN BE REMOVED ANYTIME! Load and use TensorFlow Lite model from `model.tflite` file in SavedModel directory instead of the TensorFlow model from `saved_model.pb` file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10QdGiGkIeDa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "40713eaa-37eb-46f7-e581-07bd06b19f61"
      },
      "source": [
        "%%bash --bg\n",
        "nohup tensorflow_model_server --rest_api_port=8501 --model_name=fashion_mnist --model_base_path=\"/content/save_model\" > server.log 2>&1"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 13 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbvvYBhxQT-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5d3893ee-9676-4447-9469-e16aa1bc28b8"
      },
      "source": [
        "! cat server.log"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKEIzbnSI7Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction_function\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "def _prediction(data):\n",
        "    headers = {\"content-type\": \"application/json\"}\n",
        "    json_response = requests.post('http://localhost:8501/v1/models/fashion_mnist:predict', data=data, headers=headers)\n",
        "    json_result = json.loads(json_response.text)\n",
        "    return json_result[\"predictions\"]\n",
        "\n",
        "def _prepare_image(num):\n",
        "    image = x_test[num]\n",
        "    plt.imshow(image.reshape(28, 28),cmap=\"gray\")\n",
        "    plt.plot()\n",
        "    image = np.expand_dims(image, 0)\n",
        "    return json.dumps({\"signature_name\": \"serving_default\", \"instances\": image.tolist()})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAPBRUUdLRJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "66a91769-6655-42e3-dec9-6daa8c9c8cf8"
      },
      "source": [
        "image_json = _prepare_image(111)\n",
        "image_pred = _prediction(image_json)\n",
        "class_names[np.argmax(image_pred[0])]"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sandal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANV0lEQVR4nO3dXaxV5Z3H8d8PpL5Qo/gGSJ0RUa9M\ntA1BL8ykk0kJeiESjZbEBDI1pxd17HhV0kmspsGQOu1cktBoykw6NvWdNOO0VptaL2w8oqMoodoK\nFnLkaBiU+obAvxdn0RzxrGdt9tva+v9+kpOz9/rvtdfD2ufHXms9a63HESEAn3+z2m4AgOEg7EAS\nhB1IgrADSRB2IIkThrkw2xz6BwYsIjzT9J6+2W2vsL3D9mu21/XyXgAGy932s9ueLekPkr4mabek\nZyWtjohXCvPwzQ4M2CC+2ZdJei0i/hQRByX9TNLKHt4PwAD1EvZFkv487fnuaton2B6zPW57vIdl\nAejRwA/QRcQmSZskNuOBNvXyzb5H0nnTnn+pmgZgBPUS9mclXWR7se0vSPq6pC39aRaAfut6Mz4i\nDtm+RdIvJc2WdG9EvNy3lgHoq6673rpaGPvswMAN5KQaAJ8dhB1IgrADSRB2IAnCDiRB2IEkCDuQ\nBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7\nkARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkuh6fXZJs75R0QNJhSYciYmk/GgWg/3oKe+UfI+Lt\nPrwPgAFiMx5Iotewh6Rf2X7O9thML7A9Znvc9niPywLQA0dE9zPbiyJij+1zJD0u6V8i4qnC67tf\nGICORIRnmt7TN3tE7Kl+T0p6WNKyXt4PwOB0HXbbc22fevSxpOWStvWrYQD6q5ej8fMlPWz76Pv8\nd0T8b19aBaDvetpnP+6Fsc8ODNxA9tkBfHYQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2\nIAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQ\ndiAJwg4kQdiBJAg7kEQvQzYDad16663F+saNG4v1jz/+uLZWDYNeq9uRlxu/2W3fa3vS9rZp086w\n/bjtV6vf87paOoCh6WQz/ieSVhwzbZ2kJyLiIklPVM8BjLDGsEfEU5L2HTN5paTN1ePNkq7tc7sA\n9Fm3++zzI2KievympPl1L7Q9Jmmsy+UA6JOeD9BFRNiuPWIQEZskbZKk0usADFa3XW97bS+UpOr3\nZP+aBGAQug37FklrqsdrJD3an+YAGJTGzXjb90n6qqSzbO+W9D1JGyT93PY3JO2SdEOnCyz1IXbb\nfyhJs2aV/986cuRI1+/dpKlfdMWKYzszPmnZsmXF+p133nncbfosGFR/cicuvvjiYn3Dhg3F+pIl\nS4r1c889t1hft66+A6vpb/nw4cPFep3GsEfE6prSP3W1RACt4HRZIAnCDiRB2IEkCDuQBGEHkvAg\nuzc+tbCGM+ja7Io588wzi/XrrruutnbppZcW533nnXeK9QULFhTrjz32WLF+//33F+sls2fPLtab\nuiwH+Zn0+vdQ+sxuv/324rxN3V9vvPFGsb5q1api/eDBg7W1ps+kqestImZccXyzA0kQdiAJwg4k\nQdiBJAg7kARhB5Ig7EASI9XP3ovLL7+8WL/mmmuK9UsuuaRYP3ToUG3t+eefL8771ltvFeunnHJK\nsb58+fJi/aqrrirWS5r6sgep17+9pvVy11131dZKn6ckzZ07t1i//vrri/UdO3YU63PmzKmtlW4z\n3Qn62YHkCDuQBGEHkiDsQBKEHUiCsANJEHYgiaEP2Vy6Vve2224rznvhhRfW1k466aTivLt27SrW\nm64ZP+2002prixcvLs7bdNvh3bt3d71sqdzn+8ADDxTnHeZ5Fser6fyB9evXF+sfffRRbe2KK64o\nznvjjTcW60396E3nL5SuSe/1evY6fLMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIjdT371q1bi/OX\n+sr3799fnPfDDz8s1g8cOFCsT05O1tbee++94ryla5el5n7Vc845p1g/++yza2s333xzcd42rV5d\nN0DwlLVr1xbrpXuvS+VzCG666abivO+++26xPsq6vp7d9r22J21vmzbtDtt7bL9Q/Vzdz8YC6L9O\nNuN/ImnFDNP/IyIuq37+p7/NAtBvjWGPiKck7RtCWwAMUC8H6G6x/WK1mT+v7kW2x2yP2x7vYVkA\netRt2DdKWiLpMkkTkn5Y98KI2BQRSyNiaZfLAtAHXYU9IvZGxOGIOCLpx5KW9bdZAPqtq7DbXjjt\n6SpJ2+peC2A0NPaz275P0lclnSVpr6TvVc8vkxSSdkr6ZkRMNC3s5JNPjtI16Y888khx/omJ+kWc\nfvrpxXmbrndv6gs/8cQTi/WSpnuUN40F3nSOwAUXXND1vE1ta/r7aFpvpfVe+jw7WXbT+Qmlfvhn\nnnmmOO+CBQuK9SYffPBBsV76XJruX3D33XfX1p5++mnt379/xn72xptXRMRMZz7c0zQfgNHC6bJA\nEoQdSIKwA0kQdiAJwg4kMdRbSc+aNavYFfPkk08W53/99ddra710w0jl2w5L0vvvv9/1sufNqz2b\nWFLzEL3bt28v1kvrtHT5q9T87276tzV13ZWGq266rLjX7tJSd2zTZcO9dLVK0gknlKNVqjd9ZqVL\nvUufJ9/sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5DESN1KGkDvur6VNIDPB8IOJEHYgSQIO5AEYQeS\nIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTSG3fZ5tn9j+xXbL9v+djX9\nDNuP2361+l0eCQFAqxrvVGN7oaSFEbHV9qmSnpN0raS1kvZFxAbb6yTNi4jvNLwXd6oBBqzrO9VE\nxEREbK0eH5C0XdIiSSslba5etllT/wEAGFHHNdab7fMlfVnS7yXNj4iJqvSmpPk184xJGuu+iQD6\noeMbTtr+oqTfSlofEQ/Z3h8Rp0+r/39EFPfb2YwHBq+nG07aniPpQUk/jYiHqsl7q/35o/v1k/1o\nKIDB6ORovCXdI2l7RPxoWmmLpDXV4zWSHu1/8wD0SydH46+U9DtJL0k6Uk3+rqb2238u6e8k7ZJ0\nQ0Tsa3gvNuOBAavbjGeQCOBzhkEigOQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARh\nB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE\nYQeSIOxAEoQdSKKT8dnPs/0b26/Yftn2t6vpd9jeY/uF6ufqwTcXQLc6GZ99oaSFEbHV9qmSnpN0\nraQbJP0lIv6944UxZDMwcHVDNp/QwYwTkiaqxwdsb5e0qL/NAzBox7XPbvt8SV+W9Ptq0i22X7R9\nr+15NfOM2R63Pd5TSwH0pHEz/m8vtL8o6beS1kfEQ7bnS3pbUkj6vqY29f+54T3YjAcGrG4zvqOw\n254j6ReSfhkRP5qhfr6kX0TEJQ3vQ9iBAasLeydH4y3pHknbpwe9OnB31CpJ23ptJIDB6eRo/JWS\nfifpJUlHqsnflbRa0mWa2ozfKemb1cG80nvxzQ4MWE+b8f1C2IHB63ozHsDnA2EHkiDsQBKEHUiC\nsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJxhtO9tnbknZNe35WNW0UjWrbRrVd\nEm3rVj/b9vd1haFez/6phdvjEbG0tQYUjGrbRrVdEm3r1rDaxmY8kARhB5JoO+ybWl5+yai2bVTb\nJdG2bg2lba3uswMYnra/2QEMCWEHkmgl7LZX2N5h+zXb69poQx3bO22/VA1D3er4dNUYepO2t02b\ndobtx22/Wv2ecYy9lto2EsN4F4YZb3XdtT38+dD32W3PlvQHSV+TtFvSs5JWR8QrQ21IDds7JS2N\niNZPwLD9D5L+Iuk/jw6tZfsHkvZFxIbqP8p5EfGdEWnbHTrOYbwH1La6YcbXqsV118/hz7vRxjf7\nMkmvRcSfIuKgpJ9JWtlCO0ZeRDwlad8xk1dK2lw93qypP5ahq2nbSIiIiYjYWj0+IOnoMOOtrrtC\nu4aijbAvkvTnac93a7TGew9Jv7L9nO2xthszg/nThtl6U9L8Nhszg8ZhvIfpmGHGR2bddTP8ea84\nQPdpV0bEVyRdJelb1ebqSIqpfbBR6jvdKGmJpsYAnJD0wzYbUw0z/qCkf42Id6fX2lx3M7RrKOut\njbDvkXTetOdfqqaNhIjYU/2elPSwpnY7RsneoyPoVr8nW27P30TE3og4HBFHJP1YLa67apjxByX9\nNCIeqia3vu5matew1lsbYX9W0kW2F9v+gqSvS9rSQjs+xfbc6sCJbM+VtFyjNxT1FklrqsdrJD3a\nYls+YVSG8a4bZlwtr7vWhz+PiKH/SLpaU0fk/yjp39poQ027LpD0f9XPy223TdJ9mtqs+1hTxza+\nIelMSU9IelXSryWdMUJt+y9NDe39oqaCtbCltl2pqU30FyW9UP1c3fa6K7RrKOuN02WBJDhAByRB\n2IEkCDuQBGEHkiDsQBKEHUiCsANJ/BWvm2rFpP02WgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xv6tf_hLVRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4f0281bb-777b-4e48-d681-7adce4fa1f2a"
      },
      "source": [
        "! tail server.log"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2xUB1FMLCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4H3wvDiNJ95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kHIuAq3OE3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}